import streamlit as st

from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback

# Webãƒšãƒ¼ã‚¸è¦ç´„ç”¨
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse


from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Qdrant
from langchain.chains import RetrievalQA

from langchain.schema import (
    SystemMessage,  # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    HumanMessage,  # äººé–“ã®è³ªå•
    AIMessage  # ChatGPTã®è¿”ç­”
)

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

# ãƒ™ã‚¯ãƒˆãƒ«DBã®ä¿å­˜å ´æ‰€æŒ‡å®šã€‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã€ãƒ¡ãƒ¢ãƒªã€ãƒ•ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹(Qdrant Cloud)ã€å„ç¨®On-premise(è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼, AWS, GCPãªã©)ã‹ã‚‰é¸æŠ
QDRANT_PATH = "./local_qdrant"  # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
COLLECTION_NAME = "my_collection_QA"


################################################################
## å…¨ãƒšãƒ¼ã‚¸å…±é€š
def init_page():
    # ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®è¨­å®š
    st.set_page_config(
        page_title="ç”ŸæˆAIã‚¢ãƒ—ãƒª",
        page_icon="ğŸš€"
    )
    st.header("ç”ŸæˆAIã‚¢ãƒ—ãƒªğŸ“„")
    st.caption("é•·ã„è³‡æ–™ãƒ»å‹•ç”»ã‚’è¦ç´„ã™ã‚‹ã¨ãã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§:blue[GPT-3.5-16k]ã‚’é¸æŠã—ã¦ã­")
    st.sidebar.title("Navigations")
    # st.session_state.model = "gpt-3.5-turbo-16k-0613"
    st.session_state.costs = []
    st.session_state.prompt_tokens = []
    st.session_state.answer_tokens = []


def select_model():
    model = st.sidebar.selectbox(
        "ä»Šå›ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯:",
        ("GPT-3.5", "GPT-3.5-16k"))
    if model == "GPT-3.5":
        st.session_state.model_name = "gpt-3.5-turbo-0613"
    else:
        st.session_state.model_name = "gpt-3.5-turbo-16k-0613"

    # st.session_state.emb_model_name = "gpt-3.5-turbo-16k-0613"  # emb_model_nameã‚’åˆæœŸåŒ–ã™ã‚‹

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
    # åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.1ã¨ã™ã‚‹
    temperature = st.sidebar.slider(
        "å›ç­”ã®å‰µé€ æ€§:", min_value=0.0, max_value=1.0, value=0.0, step=0.01)

    st.session_state.max_token = OpenAI.modelname_to_contextsize(
        st.session_state.model_name) - 300
    # st.session_state.overlap = st.sidebar.slider(
    #     "ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡æ–‡å­—æ•°:", min_value=0, max_value=100, value=0, step=10)
    return ChatOpenAI(temperature=temperature, model_name=st.session_state.model_name)


def calculate_costs():
    costs = st.session_state.get('costs', [])
    st.sidebar.markdown("## Costs")
    st.sidebar.markdown(f"**Total cost: ${sum(costs):.5f}**")
    for cost in costs:
        st.sidebar.markdown(f"- ${cost:.5f}")


def display_tokens():
    prompt_tokens = st.session_state.get('prompt_tokens', [])
    answer_tokens = st.session_state.get('answer_tokens', [])
    st.sidebar.markdown("## Tokens")
    st.sidebar.markdown(f"Prompt Tokens: {prompt_tokens}")
    st.sidebar.markdown(f"Completion Tokens: {answer_tokens}")


################################################################
## QAãƒãƒ£ãƒƒãƒˆç”¨
# ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸã¨ãã«ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´åˆæœŸåŒ–ã¨åŒã˜å‡¦ç†ã‚’èµ°ã‚‰ã›ã‚‹ã¨ã€å±¥æ­´ã‚’æ¶ˆã™
def init_messages():  # åˆæœŸæ¡ä»¶è¨­å®š
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    if clear_button or "messages" not in st.session_state:
        st.session_state.messages = [
            SystemMessage(content="çµ¶å¯¾ã«é–¢è¥¿å¼ã§è¿”ç­”ã—ã¦ãã ã•ã„.")
        ]
        st.session_state.costs = []


def display_chat_history():  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´è¡¨ç¤º
    messages = st.session_state.get('messages', [])
    for message in messages:
        if isinstance(message, AIMessage):
            with st.chat_message('assistant'):
                st.markdown(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message('user'):
                st.markdown(message.content)
        else:  # isinstance(message, SystemMessage):
            st.write(f"System message: {message.content}")


def get_answer(llm, messages):  # å›ç­”çµæœã€ã‚³ã‚¹ãƒˆãƒ»ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¶ˆè²»é‡ã®ç®—å‡ºã‚’è¡Œã†é–¢æ•°
    with get_openai_callback() as cb:
        answer = llm(messages)
    return answer.content, cb.total_cost, cb.prompt_tokens, cb.completion_tokens

################################################################
## Webãƒšãƒ¼ã‚¸è¦ç´„ã‚µã‚¤ãƒˆç”¨


def get_url_input():  # URLå—ã‘å–ã‚Šé–¢æ•°
    url = st.chat_input("URL: ", key="input")
    return url


def validate_url(url):  # URLãŒæ­£ã—ã„ã‹æ¤œè¨¼ã™ã‚‹é–¢æ•°
    try:
        result = urlparse(url)
        return all([result.scheme, result.netloc])
    except ValueError:
        return False


def get_content(url):  # ãƒšãƒ¼ã‚¸ã®å†…å®¹å–å¾—
    try:
        with st.spinner("è¨˜äº‹èª­ã¿è¾¼ã¿ä¸­ ..."):
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            # fetch text from main (change the below code to filter page)
            # main, article, bodyã‚¿ã‚°ã‚’å–å¾—
            if soup.main:
                return soup.main.get_text()
            elif soup.article:
                return soup.article.get_text()
            else:
                return soup.body.get_text()
    except:
        st.write('something wrong')
        return None


def build_prompt(content, n_chars=300):  # è¦ç´„æŒ‡ç¤ºPromptã®æ§‹ç¯‰
    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒé•·ã„å ´åˆã¯å…ˆé ­1000æ–‡å­—ã‚’åˆ©ç”¨ã—ã¦ã€é©å½“ã«çœç•¥ã—ã¦ã‚‹
    return f"""
            ä»¥ä¸‹ã¯ã¨ã‚ã‚‹Webãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ã™ã€‚å†…å®¹ã‚’ä»¥ä¸‹ã®æ¡ä»¶ã‚’å®ˆã£ã¦ã‚ã‹ã‚Šã‚„ã™ãè¦ç´„ã—ã¦ãã ã•ã„ã€‚
            - è¦ç´„ã¯{n_chars}æ–‡å­—ã§ã€‚
            - ç®‡æ¡æ›¸ã
            - é–¢è¥¿å¼
            - ä¸­å­¦ç”Ÿå‘ã‘
            ========

            {content[:1000]}
            ========
            """

################################################################


def get_pdf_text():  # PDFã‹ã‚‰textæŠ½å‡º
    uploaded_file = st.file_uploader(
        label='PDFã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ğŸ“',
        type='pdf',  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’è¨±å¯ã™ã‚‹æ‹¡å¼µå­ (è¤‡æ•°è¨­å®šå¯)
        accept_multiple_files=False  # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’è¨±å¯ã™ã‚‹ã‹ãƒ•ãƒ©ã‚°
    )
    if uploaded_file:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºã¯200MBã¾ã§. Streamlitã®ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§server.maxUploadSizeè¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å¤‰æ›´å¯èƒ½
        pdf_reader = PdfReader(uploaded_file)
        text = '\n\n'.join([page.extract_text() for page in pdf_reader.pages])
        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            model_name="text-embedding-ada-002",
            # é©åˆ‡ãª chunk size ã¯è³ªå•å¯¾è±¡ã®PDFã«ã‚ˆã£ã¦å¤‰ã‚ã‚‹ãŸã‚èª¿æ•´ãŒå¿…è¦
            # å¤§ããã—ã™ãã‚‹ã¨è³ªå•å›ç­”æ™‚ã«è‰²ã€…ãªç®‡æ‰€ã®æƒ…å ±ã‚’å‚ç…§ã™ã‚‹ã“ã¨ãŒã§ããªã„
            # é€†ã«å°ã•ã™ãã‚‹ã¨ä¸€ã¤ã®chunkã«ååˆ†ãªã‚µã‚¤ã‚ºã®æ–‡è„ˆãŒå…¥ã‚‰ãªã„
            chunk_size=250,
            chunk_overlap=0,
        )
        return text_splitter.split_text(text)
    else:
        return None


def load_qdrant():  # ãƒ™ã‚¯ãƒˆãƒ«DBã‚’æ“ä½œã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æº–å‚™ãƒ»è¨­å®š
    client = QdrantClient(path=QDRANT_PATH)

    # ã™ã¹ã¦ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆDBã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¿ãŸã„ãªã‚‚ã®ï¼‰åã‚’å–å¾—
    collections = client.get_collections().collections
    collection_names = [collection.name for collection in collections]

    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°ä½œæˆ
    if COLLECTION_NAME not in collection_names:
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã€æ–°ã—ãä½œæˆã—ã¾ã™
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
        )
        print('collection created')

    # ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚„Documentã®å†…å®¹ã‚’embeddingsã§ä¸ãˆãŸãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦EmbeddingåŒ–ã™ã‚‹
    # ãƒ™ã‚¯ãƒˆãƒ«DBã®clientã‚’ç”¨ã„ã¦ã€ç”Ÿæˆã—ãŸEmbeddingã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã®collection_nameã«ä¿ç®¡
    return Qdrant(
        client=client,
        collection_name=COLLECTION_NAME,
        embeddings=OpenAIEmbeddings()
    )


def build_vector_store(pdf_text):  # PDFã®ãƒ†ã‚­ã‚¹ãƒˆã‚’Embeddingã«ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«DBã«ä¿å­˜

    qdrant = load_qdrant()
    qdrant.add_texts(pdf_text)
    # qdrant.client.close()  # Qdrantã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’çµ‚äº†ã•ã›ã‚‹


def pdf_upload_and_build_vector_db():  # PDFã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€œãƒ™ã‚¯ãƒˆã‚ŠDBæ§‹ç¯‰
    st.title("PDF(s)ç‹¬è‡ªQAã‚µã‚¤ãƒˆ")
    llm = select_model()
    container = st.container()
    with container:

        pdf_text = get_pdf_text()
        if pdf_text:
            with st.spinner("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ ..."):
                build_vector_store(pdf_text)
            st.write("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            st.write("PDFã®å†…å®¹ã«ã¤ã„ã¦è³ªå•ã—ã¦ãã ã•ã„ã€‚")


def build_qa_model(llm):
    qdrant = load_qdrant()
    retriever = qdrant.as_retriever(
        search_type="similarity",  # "mmr",  "similarity_score_threshold" ãªã©ã‚‚ã‚ã‚‹
        search_kwargs={"k": 10}  # æ–‡æ›¸ã‚’ä½•å€‹å–å¾—ã™ã‚‹ã‹ (default: 4)
    )

    # è¿½åŠ ã®æ–‡è„ˆæƒ…å ±ã‚’æ´»ã‹ã—ãŸè³ªå•å¿œç­”ã‚’å®Ÿç¾
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # å›ç­”ã®ç”Ÿæˆæ–¹æ³•ã‚’èª¿æ•´
        retriever=retriever,
        return_source_documents=False,  # Trueãªã‚‰å‚ç…§ã—ãŸæ–‡è„ˆæƒ…å ±ã‚’ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å«ã‚
        verbose=True
    )


def ask(qa, query):
    with get_openai_callback() as cb:
        # response = qa.run(query)
        # answer = response['result']
        response = qa(query)
        answer = response['result']

    return answer, cb.total_cost, cb.prompt_tokens, cb.completion_tokens


def pdf_ask_my_pdf():
    # st.title("PDF(s)ç‹¬è‡ªQAã‚µã‚¤ãƒˆ")
    llm = select_model()
    container = st.container()
    response_container = st.container()

    with container:
        query = st.text_input("Query: ", key="input")
        if not query:
            answer = None
        else:
            qa = build_qa_model(llm)
            if qa:
                with st.spinner("ChatGPT is typing ..."):
                    answer, cost, prompt_token, answer_token = ask(qa, query)
                st.session_state.costs.append(cost)
                st.session_state.prompt_tokens.append(prompt_token)
                st.session_state.answer_tokens.append(answer_token)
            else:
                answer = None

        if answer:
            with response_container:
                st.markdown("## Answer")
                st.write(answer)


def main():
    init_page()
    # n_chars = select_n_chars()

    selection = st.sidebar.radio(
        "ãƒšãƒ¼ã‚¸", ["ãƒãƒ£ãƒƒãƒˆQAã‚µã‚¤ãƒˆ", "Webãƒšãƒ¼ã‚¸è¦ç´„ã‚µã‚¤ãƒˆ", "Youtubeè¦ç´„ã‚µã‚¤ãƒˆ", "PDF(s)ç‹¬è‡ªQAã‚µã‚¤ãƒˆ"])

    if selection == "ãƒãƒ£ãƒƒãƒˆQAã‚µã‚¤ãƒˆ":
        st.header("ãƒãƒ£ãƒƒãƒˆQAã‚µã‚¤ãƒˆğŸ—£ï¸")
        init_messages()
        llm = select_model()
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ç›£è¦–
        if user_input := st.chat_input("èããŸã„ã“ã¨ã‚’å…¥åŠ›ã—ã¦ã­ï¼"):
            st.session_state.messages.append(HumanMessage(content=user_input))
            with st.spinner("ChatGPT is typing ..."):
                answer, cost, prompt_token, answer_token = get_answer(
                    llm, st.session_state.messages)
            st.session_state.messages.append(AIMessage(content=answer))
            st.session_state.costs.append(cost)
            st.session_state.prompt_tokens.append(prompt_token)
            st.session_state.answer_tokens.append(answer_token)

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®è¡¨ç¤º
        display_chat_history()

    elif selection == "Webãƒšãƒ¼ã‚¸è¦ç´„ã‚µã‚¤ãƒˆ":
        st.header("Webãƒšãƒ¼ã‚¸è¦ç´„ã‚µã‚¤ãƒˆâœï¸")
        st.subheader("Promptå†…å®¹")
        st.write("""
            - è¦ç´„ã¯300æ–‡å­—ã§ã€‚
            - ç®‡æ¡æ›¸ã
            - é–¢è¥¿å¼
            - ä¸­å­¦ç”Ÿå‘ã‘
            """)

        llm = select_model()
        # n_chars = select_n_chars()
        init_messages()

        container = st.container()
        response_container = st.container()

        with container:
            url = get_url_input()
            is_valid_url = validate_url(url)
            if not is_valid_url:
                st.subheader('  ğŸ‘‡Webãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚')
                answer = None
            else:
                content = get_content(url)
                if content:
                    prompt = build_prompt(content, n_chars=300)
                    st.session_state.messages.append(
                        HumanMessage(content=prompt))
                    with st.spinner("ChatGPT is typing ..."):
                        answer, cost, prompt_token, answer_token = get_answer(
                            llm, st.session_state.messages)
                    # st.session_state.messages.append(AIMessage(content=answer))
                    st.session_state.costs.append(cost)
                    st.session_state.prompt_tokens.append(prompt_token)
                    st.session_state.answer_tokens.append(answer_token)
                else:
                    answer = None

        if answer:
            with response_container:
                st.markdown("## Summary")
                st.write("è¦ç´„æ–‡å­—æ•°ï¼š", n_chars=300)
                st.write(answer)
                # st.markdown("---")
                # st.markdown("## Original Text")
                # st.write(content)

    elif selection == "Youtubeè¦ç´„ã‚µã‚¤ãƒˆ":
        st.header("Youtube Summarizer AppsğŸ¥")
        st.subheader("15åˆ†ä»¥ä¸Šã®å‹•ç”»ã‚’è¦ç´„ã™ã‚‹ã¨ãã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§:blue[GPT-3.5-16k]ã‚’é¸æŠã—ã¦ã­")

    else:
        pdf_upload_and_build_vector_db()
        pdf_ask_my_pdf()

    costs = st.session_state.get('costs', [])
    prompt_tokens = st.session_state.get('prompt_tokens', [])
    answer_tokens = st.session_state.get('answer_tokens', [])

    # æ–™é‡‘è¨ˆç®—
    calculate_costs()

    # Prompt Tokens, Completion Tokensã®è¡¨ç¤º
    display_tokens()


if __name__ == "__main__":
    main()
