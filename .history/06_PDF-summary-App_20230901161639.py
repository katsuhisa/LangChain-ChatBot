# OpenAI Embeddings APIã§ã®è²»ç”¨ï¼šã€ŒPDFã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«ä¿å­˜ã™ã‚‹ã¨ãã€ã€Œè³ªå•ã‚’æŠ•ã’ã‚‹ã¨ãã€ã®2ã‹æ‰€ã§ç™ºç”Ÿ

# Vector DBã‚’Pineconeã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£
# è¤‡æ•°PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ä¿®æ­£
# ConversationalRetrievalChainæ©Ÿèƒ½ã‚’ä½¿ãˆã°ã€QAãƒ©ãƒªãƒ¼ã‚’ç¹°ã‚Šè¿”ã™ã“ã¨ãŒå¯èƒ½ã«ã€‚

import streamlit as st

from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import get_openai_callback

from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Qdrant
from langchain.chains import RetrievalQA


from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

# ãƒ™ã‚¯ãƒˆãƒ«DBã®ä¿å­˜å ´æ‰€æŒ‡å®šã€‚ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã€ãƒ¡ãƒ¢ãƒªã€ãƒ•ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹(Qdrant Cloud)ã€å„ç¨®On-premise(è‡ªç¤¾ã‚µãƒ¼ãƒãƒ¼, AWS, GCPãªã©)ã‹ã‚‰é¸æŠ
QDRANT_PATH = "./local_qdrant"  # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜
COLLECTION_NAME = "my_collection_QA"


def init_page():
    # ã‚¦ã‚§ãƒ–ãƒšãƒ¼ã‚¸ã®è¨­å®š
    st.set_page_config(
        page_title="PDF Summarizer Apps",
        page_icon="ğŸš€"
    )
    st.header("PDF Summarizer AppsğŸ“„")
    st.subheader("é•·ã„è³‡æ–™ã‚’è¦ç´„ã™ã‚‹ã¨ãã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§:blue[GPT-3.5-16k]ã‚’é¸æŠã—ã¦ã­")
    st.sidebar.title("Navigations")
    # st.session_state.model = "gpt-3.5-turbo-16k-0613"
    st.session_state.costs = []
    st.session_state.prompt_tokens = []
    st.session_state.answer_tokens = []


def select_model():
    model = st.sidebar.selectbox(
        "ä»Šå›ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯:",
        ("GPT-3.5", "GPT-3.5-16k", "GPT-4"),
        key="model_selection")
    if model == "GPT-3.5":
        st.session_state.model_name = "gpt-3.5-turbo-0613"
    elif model == "GPT-3.5-16k":
        st.session_state.model_name = "gpt-3.5-turbo-16k-0613"
    else:
        st.session_state.model_name = "gpt-4"
    st.session_state.emb_model_name = "gpt-3.5-turbo-16k-0613"  # emb_model_nameã‚’åˆæœŸåŒ–ã™ã‚‹

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’è¿½åŠ ã—ã€temperatureã‚’0ã‹ã‚‰2ã¾ã§ã®ç¯„å›²ã§é¸æŠå¯èƒ½ã«ã™ã‚‹
    # åˆæœŸå€¤ã¯0.0ã€åˆ»ã¿å¹…ã¯0.1ã¨ã™ã‚‹
    temperature = st.sidebar.slider(
        "å›ç­”ã®å‰µé€ æ€§:", min_value=0.0, max_value=1.0, value=0.0, step=0.01)

    st.session_state.max_token = OpenAI.modelname_to_contextsize(
        st.session_state.model_name) - 300
    # st.session_state.overlap = st.sidebar.slider(
    #     "ãƒãƒ£ãƒ³ã‚¯é–“ã®é‡è¤‡æ–‡å­—æ•°:", min_value=0, max_value=100, value=0, step=10)
    return ChatOpenAI(temperature=temperature, model_name=st.session_state.model_name)


def calculate_costs():
    costs = st.session_state.get('costs', [])
    st.sidebar.markdown("## Costs")
    st.sidebar.markdown(f"**Total cost: ${sum(costs):.5f}**")
    for cost in costs:
        st.sidebar.markdown(f"- ${cost:.5f}")


def display_tokens():
    prompt_tokens = st.session_state.get('prompt_tokens', [])
    answer_tokens = st.session_state.get('answer_tokens', [])
    st.sidebar.markdown("## Tokens")
    st.sidebar.markdown(f"Prompt Tokens: {prompt_tokens}")
    st.sidebar.markdown(f"Completion Tokens: {answer_tokens}")


def get_pdf_text():  # PDFã‹ã‚‰textæŠ½å‡º
    uploaded_file = st.file_uploader(
        label='PDFã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ğŸ“',
        type='pdf',  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’è¨±å¯ã™ã‚‹æ‹¡å¼µå­ (è¤‡æ•°è¨­å®šå¯)
        accept_multiple_files=False  # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’è¨±å¯ã™ã‚‹ã‹ãƒ•ãƒ©ã‚°
    )
    if uploaded_file:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºã¯200MBã¾ã§. Streamlitã®ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§server.maxUploadSizeè¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å¤‰æ›´å¯èƒ½
        pdf_reader = PdfReader(uploaded_file)
        text = '\n\n'.join([page.extract_text() for page in pdf_reader.pages])
        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
            model_name=st.session_state.emb_model_name,
            # é©åˆ‡ãª chunk size ã¯è³ªå•å¯¾è±¡ã®PDFã«ã‚ˆã£ã¦å¤‰ã‚ã‚‹ãŸã‚èª¿æ•´ãŒå¿…è¦
            # å¤§ããã—ã™ãã‚‹ã¨è³ªå•å›ç­”æ™‚ã«è‰²ã€…ãªç®‡æ‰€ã®æƒ…å ±ã‚’å‚ç…§ã™ã‚‹ã“ã¨ãŒã§ããªã„
            # é€†ã«å°ã•ã™ãã‚‹ã¨ä¸€ã¤ã®chunkã«ååˆ†ãªã‚µã‚¤ã‚ºã®æ–‡è„ˆãŒå…¥ã‚‰ãªã„
            chunk_size=250,
            chunk_overlap=0,
        )
        return text_splitter.split_text(text)
    else:
        return None


def load_qdrant():  # ãƒ™ã‚¯ãƒˆãƒ«DBã‚’æ“ä½œã™ã‚‹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æº–å‚™ãƒ»è¨­å®š
    client = QdrantClient(path=QDRANT_PATH)

    # ã™ã¹ã¦ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆDBã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¿ãŸã„ãªã‚‚ã®ï¼‰åã‚’å–å¾—
    collections = client.get_collections().collections
    collection_names = [collection.name for collection in collections]

    # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã‘ã‚Œã°ä½œæˆ
    if COLLECTION_NAME not in collection_names:
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã€æ–°ã—ãä½œæˆã—ã¾ã™
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
        )
        print('collection created')

    # ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚„Documentã®å†…å®¹ã‚’embeddingsã§ä¸ãˆãŸãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦EmbeddingåŒ–ã™ã‚‹
    # ãƒ™ã‚¯ãƒˆãƒ«DBã®clientã‚’ç”¨ã„ã¦ã€ç”Ÿæˆã—ãŸEmbeddingã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã®collection_nameã«ä¿ç®¡
    return Qdrant(
        client=client,
        collection_name=COLLECTION_NAME,
        embeddings=OpenAIEmbeddings()
    )


def build_vector_store(pdf_text):  # PDFã®ãƒ†ã‚­ã‚¹ãƒˆã‚’Embeddingã«ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«DBã«ä¿å­˜
    qdrant = load_qdrant()
    qdrant.add_texts(pdf_text)


def page_pdf_upload_and_build_vector_db():  # PDFã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€œãƒ™ã‚¯ãƒˆã‚ŠDBæ§‹ç¯‰
    st.title("PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ãƒšãƒ¼ã‚¸")
    container = st.container()
    with container:
        # ã•ã£ãä½œã£ãŸã‚‚ã®
        pdf_text = get_pdf_text()
        if pdf_text:
            with st.spinner("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­ ..."):
                build_vector_store(pdf_text)


def build_qa_model(llm):
    qdrant = load_qdrant()
    retriever = qdrant.as_retriever(
        search_type="similarity",  # "mmr",  "similarity_score_threshold" ãªã©ã‚‚ã‚ã‚‹
        search_kwargs={"k": 10}  # æ–‡æ›¸ã‚’ä½•å€‹å–å¾—ã™ã‚‹ã‹ (default: 4)
    )

    # è¿½åŠ ã®æ–‡è„ˆæƒ…å ±ã‚’æ´»ã‹ã—ãŸè³ªå•å¿œç­”ã‚’å®Ÿç¾
    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",  # å›ç­”ã®ç”Ÿæˆæ–¹æ³•ã‚’èª¿æ•´
        retriever=retriever,
        return_source_documents=True,  # Trueãªã‚‰å‚ç…§ã—ãŸæ–‡è„ˆæƒ…å ±ã‚’ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å«ã‚
        verbose=True
    )


def ask(qa, query):
    with get_openai_callback() as cb:
        answer = qa.run(query)

    return answer, cb.total_cost


def page_ask_my_pdf():
    st.title("PDF(s)ç‹¬è‡ªQAã‚µã‚¤ãƒˆ")
    llm = select_model()
    container = st.container()
    response_container = st.container()

    with container:
        query = st.text_input("Query: ", key="input")
        if not query:
            answer = None
        else:
            qa = build_qa_model(llm)
            if qa:
                with st.spinner("ChatGPT is typing ..."):
                    answer, cost = ask(qa, query)
                st.session_state.costs.append(cost)
            else:
                answer = None

        if answer:
            with response_container:
                st.markdown("## Answer")
                st.write(answer)


def main():
    init_page()
    llm = select_model()
    # n_chars = select_n_chars()
    # init_messages()

    selection = st.sidebar.radio("ãƒšãƒ¼ã‚¸", ["PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ãƒšãƒ¼ã‚¸", "PDF(s)ç‹¬è‡ªQAã‚µã‚¤ãƒˆ"])
    if selection == "PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ç”¨ãƒšãƒ¼ã‚¸":
        page_pdf_upload_and_build_vector_db()
    elif selection == "PDF(s)ç‹¬è‡ªQAã‚µã‚¤ãƒˆ":
        page_ask_my_pdf()

    costs = st.session_state.get('costs', [])
    prompt_tokens = st.session_state.get('prompt_tokens', [])
    answer_tokens = st.session_state.get('answer_tokens', [])

    # æ–™é‡‘è¨ˆç®—
    calculate_costs()

    # Prompt Tokens, Completion Tokensã®è¡¨ç¤º
    display_tokens()


if __name__ == "__main__":
    main()
